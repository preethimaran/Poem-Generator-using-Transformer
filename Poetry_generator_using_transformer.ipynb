{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preethimaran/Poem-Generator-using-Transformer/blob/main/Poetry_generator_using_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqUAWcuvsjFt"
      },
      "source": [
        "First we will install and import the required directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81ksDbAqmj8d",
        "outputId": "bbf1cc7c-e50a-44ba-da3d-c479edd494a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy torch matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07eErvzDkHIh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F # to access the softmax function we will use while calculating attention\n",
        "\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import lightning as L\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4E0NRHCtRHV"
      },
      "source": [
        "Next we have to set the device on which computations are to be performed. We will set it to be cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjGkRH67taX-"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrbCp1jKmj8f",
        "outputId": "468f975d-9b25-4d30-e62f-323bec17de10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vggkSeoNwwc-"
      },
      "source": [
        "Let us now download and use the poetry dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ltl1VR5wvcS",
        "outputId": "a0308bed-5c1c-4b40-ce8f-cacc84f08c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the Poetry Foundation dataset available in Kaggle. For that first we need to upload the kaggle.json file"
      ],
      "metadata": {
        "id": "OY3wrdeK2z9Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "5ho-dGFQxanN",
        "outputId": "a76ce1df-4779-4bf1-d794-de2a408cf59d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1616d3fa-ffe6-4d7c-a63b-ee03ef61e16f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1616d3fa-ffe6-4d7c-a63b-ee03ef61e16f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle (1).json': b'{\"username\":\"preethimaran\",\"key\":\"00c0a7e12937ea5ea084dd931b75d7df\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # This will prompt you to select kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8vaQ87-yQBC"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzipping the csv file"
      ],
      "metadata": {
        "id": "XL2w9tOo3DmJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5a07G69yXEM",
        "outputId": "6e0ab502-c0b5-4409-cf9f-8e17d719b093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/tgdivy/poetry-foundation-poems\n",
            "License(s): GNU Affero General Public License 3.0\n",
            "poetry-foundation-poems.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  poetry-foundation-poems.zip\n",
            "replace poetry_dataset/PoetryFoundationData.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: poetry_dataset/PoetryFoundationData.csv  \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d tgdivy/poetry-foundation-poems\n",
        "!unzip poetry-foundation-poems.zip -d poetry_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other option is to directly download the zip file from:https://www.kaggle.com/datasets/tgdivy/poetry-foundation-poems\n",
        "\n",
        "Unzip the csv file and create the directory under the folder containing the ipynb file by running the below:\n",
        "\n",
        "  *import os*\n",
        "\n",
        "  *os.makedirs(\"poetry_dataset\", exist_ok=True)*\n",
        "\n",
        "And place the csv file in the above directory"
      ],
      "metadata": {
        "id": "sIg9JPRQBLHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the file as a pandas dataframe"
      ],
      "metadata": {
        "id": "KPjDhJpM3I3l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JljbdWtyf-X",
        "outputId": "5d08aaf6-525a-43fe-9716-b840bf7ee525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                              Title  \\\n",
            "0           0  \\r\\r\\n                    Objects Used to Prop...   \n",
            "1           1  \\r\\r\\n                    The New Church\\r\\r\\n...   \n",
            "2           2  \\r\\r\\n                    Look for Me\\r\\r\\n   ...   \n",
            "3           3  \\r\\r\\n                    Wild Life\\r\\r\\n     ...   \n",
            "4           4  \\r\\r\\n                    Umbrella\\r\\r\\n      ...   \n",
            "\n",
            "                                                Poem              Poet Tags  \n",
            "0  \\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, ...  Michelle Menting  NaN  \n",
            "1  \\r\\r\\nThe old cupola glinted above the clouds,...     Lucia Cherciu  NaN  \n",
            "2  \\r\\r\\nLook for me under the hood\\r\\r\\nof that ...        Ted Kooser  NaN  \n",
            "3  \\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\n...   Grace Cavalieri  NaN  \n",
            "4  \\r\\r\\nWhen I push your button\\r\\r\\nyou fly off...      Connie Wanek  NaN  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"poetry_dataset/PoetryFoundationData.csv\")\n",
        "print(df.head(5))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAnPA2irYcV3",
        "outputId": "37afc44b-7292-4584-ba81-2436772145bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'Title', 'Poem', 'Poet', 'Tags'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at one of the poems"
      ],
      "metadata": {
        "id": "SjDIQh2e3QNA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3idC_UoApE-Y",
        "outputId": "f90c94b2-6281-44fd-f3de-e804126c552c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\r\n",
            "Dog bone, stapler,\r\r\n",
            "cribbage board, garlic press\r\r\n",
            "     because this window is loose—lacks\r\r\n",
            "suction, lacks grip.\r\r\n",
            "Bungee cord, bootstrap,\r\r\n",
            "dog leash, leather belt\r\r\n",
            "     because this window had sash cords.\r\r\n",
            "They frayed. They broke.\r\r\n",
            "Feather duster, thatch of straw, empty\r\r\n",
            "bottle of Elmer's glue\r\r\n",
            "     because this window is loud—its hinges clack\r\r\n",
            "open, clack shut.\r\r\n",
            "Stuffed bear, baby blanket,\r\r\n",
            "single crib newel\r\r\n",
            "     because this window is split. It's dividing\r\r\n",
            "in two.\r\r\n",
            "Velvet moss, sagebrush,\r\r\n",
            "willow branch, robin's wing\r\r\n",
            "     because this window, it's pane-less. It's only\r\r\n",
            "a frame of air.\r\r\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(df['Poem'].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting poems that are ≤250 words and are not empty. This is beacuse we will be finetuning a gpt-2 transformer, and that transformer can only handle shorter text lengths\n",
        "df_filtered = df[(df['Poem'].apply(lambda x: len(x.split()) <= 250)) & (df['Poem'].str.strip() != \"\")]\n",
        "df_filtered = df_filtered.copy()\n",
        "\n",
        "# Adding a word count column\n",
        "df_filtered['word_count'] = df_filtered['Poem'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Stats\n",
        "print(\"Max words in Poem:\", df_filtered['word_count'].max())\n",
        "print(\"Min words in Poem:\", df_filtered['word_count'].min())\n",
        "print(\"Average words in Poem:\", df_filtered['word_count'].mean())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxUWXLQBe8HA",
        "outputId": "a01a7d9f-6dcc-4b87-9083-190430a5d7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max words in Poem: 250\n",
            "Min words in Poem: 1\n",
            "Average words in Poem: 125.73514481934906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the number of poems"
      ],
      "metadata": {
        "id": "kX0xSncI3s_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp7xkc1qdtZZ",
        "outputId": "756ed317-96c9-47d4-dd76-e37113f0e9a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10047"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "pydguczH0u1z",
        "outputId": "f407d503-7288-489a-aaa9-084f1f96c819"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        \\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, ...\n",
              "1        \\r\\r\\nThe old cupola glinted above the clouds,...\n",
              "2        \\r\\r\\nLook for me under the hood\\r\\r\\nof that ...\n",
              "3        \\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\n...\n",
              "4        \\r\\r\\nWhen I push your button\\r\\r\\nyou fly off...\n",
              "                               ...                        \n",
              "13835    \\r\\r\\nDear Writers, I’m compiling the first in...\n",
              "13848    \\r\\r\\nThe Wise Men will unlearn your name.\\r\\r...\n",
              "13849    \\r\\r\\nWe'd  like  to  talk  with  you  about  ...\n",
              "13852    \\r\\r\\n          Philosophic\\r\\r\\nin its comple...\n",
              "13853    \\r\\r\\nDear Writers, I’m compiling the first in...\n",
              "Name: Poem, Length: 10047, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Poem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\r\\r\\nThe old cupola glinted above the clouds,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\r\\r\\nLook for me under the hood\\r\\r\\nof that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\r\\r\\nWhen I push your button\\r\\r\\nyou fly off...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13835</th>\n",
              "      <td>\\r\\r\\nDear Writers, I’m compiling the first in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13848</th>\n",
              "      <td>\\r\\r\\nThe Wise Men will unlearn your name.\\r\\r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13849</th>\n",
              "      <td>\\r\\r\\nWe'd  like  to  talk  with  you  about  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13852</th>\n",
              "      <td>\\r\\r\\n          Philosophic\\r\\r\\nin its comple...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13853</th>\n",
              "      <td>\\r\\r\\nDear Writers, I’m compiling the first in...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10047 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "df_filtered['Poem']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and importing the Transformer"
      ],
      "metadata": {
        "id": "wOWIX4EY3x9H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSOX4_Ynmj8g",
        "outputId": "a8dae08b-50db-45c7-b813-cf7cf1bb861c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krwU1iflldhQ"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# We need to use GPT2LMHead model for text generation\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Defining the tokenizer to tokenize our input\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "\n",
        "# Here we need to add any special tokens we are using, 'pad_token' is the keyword to define the [PAD] token.\n",
        "# Note: For any other tokens we use, we must give them as a list under 'additional_special_tokens'\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# tokenizer.add_special_tokens({'additional_special_tokens': ['[POEM]','[TITLE]','[TOPIC]']})\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))  # Each token will be present at an embedding index. As we have added an additional [PAD] token we must resize the embedding.\n",
        "\n",
        "# Converting the input data into PyTorch tensors. When we use tokenizer method, a dictionary is returned. The key 'input_ids' contains the data as a pytorch tensor. The key\n",
        "# attention_mask contains pytorch tensor which shows which are actual data and which are tokens\n",
        "# padding = True, will take the longest input data length for padding\n",
        "input = tokenizer(list(df['Poem']), return_tensors='pt', padding = True, truncation = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tloj_SJZzGW0",
        "outputId": "8dcf3fad-33b0-48bd-ae42-4d0521a89374"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13854, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "input['input_ids'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Hqfmq5rc-2F",
        "outputId": "09dd3d59-c60f-4069-a13a-d472b5333e1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ItemsView({'input_ids': tensor([[  201,   201,   198,  ..., 50257, 50257, 50257],\n",
              "        [  201,   201,   198,  ..., 50257, 50257, 50257],\n",
              "        [  201,   201,   198,  ..., 50257, 50257, 50257],\n",
              "        ...,\n",
              "        [  201,   201,   198,  ..., 50257, 50257, 50257],\n",
              "        [  201,   201,   198,  ..., 50257, 50257, 50257],\n",
              "        [  201,   201,   198,  ..., 50257, 50257, 50257]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]])})"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "input.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUDAOzHzd9Lq",
        "outputId": "7bcfa6aa-ad46-4992-ff73-1b3ff332897b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13854, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "input['input_ids'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ6hOrrphH7B"
      },
      "source": [
        "Next we need to create a custom Dataset for the input Data. A custom Dataset must implement three functions:\n",
        "\n",
        "\n",
        "1.   __init__: is run once when instantiating the Dataset object.\n",
        "2.   __len__ : must return the number of samples in our dataset\n",
        "1.   __getitem__ : function loads and returns a sample (tensor format) from the dataset at the given index idx\n",
        "\n",
        "\n",
        "The Dataset retrives the dataset features in required postion, one at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\n",
        "\n",
        "DataLoader is an iterable that abstracts this complexity for us in an easy API.\n",
        "\n",
        "Refer: https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsITFVuRiZy3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomPoemDataset (Dataset):\n",
        "  def __init__(self, input):\n",
        "    self.input = input\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input['input_ids'])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = {key: val[idx] for key, val in self.input.items()}\n",
        "    item['labels'] = item['input_ids']\n",
        "    return item\n",
        "\n",
        "\n",
        "custom_dataset = CustomPoemDataset(input)\n",
        "custom_loader = DataLoader(custom_dataset, batch_size =4, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E7ZKRzOrlrR"
      },
      "source": [
        "[Sanity Check] example of iterating with help of Dataloader, we are going to pass one batch to the pre-trained model to check if everything is working properly and display the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BaE4lNRpbr5",
        "outputId": "cde4b4fb-b8fa-4579-c834-f3a8524996aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.5523, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(DEVICE)\n",
        "\n",
        "for batch in custom_loader:\n",
        "  input_ids = batch['input_ids'].to(DEVICE)\n",
        "  attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "  labels = batch['labels'].to(DEVICE)\n",
        "  # print(input_ids)\n",
        "  # print(attention_mask)\n",
        "  outputs = model(input_ids, attention_mask=attention_mask,labels=labels)\n",
        "  print(outputs.loss)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ygk1p_btLe1"
      },
      "source": [
        "The Next step now is to finetune the GPT2LMHead transformer on our poetry dataset. Here we will be using an Optimizer and learning rate too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAPQTmNYmj8h",
        "outputId": "f1171ea9-c81d-4cd1-f323-28ea5952dc48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 3464/3464 [1:16:40<00:00,  1.33s/it, avg_loss=2.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss: 2.1231\n",
            "The time taken is 4600.78006029129\n",
            "Checkpoint saved at epoch 1 to the path: ./checkpoints/latest.pt\n",
            "\n",
            "Training epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 3464/3464 [1:16:48<00:00,  1.33s/it, avg_loss=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 loss: 1.1784\n",
            "The time taken is 4608.224022626877\n",
            "Checkpoint saved at epoch 2 to the path: ./checkpoints/latest.pt\n",
            "\n",
            "Training epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 3464/3464 [1:16:44<00:00,  1.33s/it, avg_loss=1.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 loss: 1.1643\n",
            "The time taken is 4604.979864835739\n",
            "Checkpoint saved at epoch 3 to the path: ./checkpoints/latest.pt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from torch.optim import AdamW\n",
        "import time\n",
        "\n",
        "optimizer = AdamW(model.parameters(),lr = 5e-6)\n",
        "\n",
        "\n",
        "# Directory to save checkpoints\n",
        "CHECKPOINT_DIR = \"./checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, path):\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    }, path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch+1} to the path: {path}\")\n",
        "\n",
        "def load_checkpoint(model, optimizer, path, DEVICE):\n",
        "    checkpoint = torch.load(path, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
        "    return start_epoch\n",
        "\n",
        "# Load from the checkpoints if they are present or start from epoch 1\n",
        "start_epoch = 0\n",
        "num_epochs = 3\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, \"latest.pt\")\n",
        "if os.path.exists(checkpoint_path):\n",
        "    start_epoch = load_checkpoint(model, optimizer, checkpoint_path, DEVICE)\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    total_loss = 0\n",
        "    number_of_batches = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"\\nTraining epoch {epoch+1}\")\n",
        "    progress_bar = tqdm(custom_loader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "    for j,batch in enumerate(progress_bar):\n",
        "        inputs = batch[\"input_ids\"].to(DEVICE)\n",
        "        labels = batch[\"labels\"].to(DEVICE)\n",
        "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print(f\"    Batch {j+1} loss: {loss.item()}\")\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        number_of_batches += 1\n",
        "\n",
        "        # Show average loss so far\n",
        "        progress_bar.set_postfix(avg_loss=total_loss/number_of_batches)\n",
        "\n",
        "    avg_loss = total_loss / number_of_batches\n",
        "    print(f\"Epoch {epoch+1} loss: {avg_loss:.4f}\")\n",
        "    print(f\"The time taken is {time.time()-start_time}\")\n",
        "\n",
        "    # Save checkpoint after each epoch\n",
        "    save_checkpoint(model, optimizer, epoch, checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model_path = os.path.join(CHECKPOINT_DIR, \"final_model.pt\")\n",
        "final_optimizer_path = os.path.join(CHECKPOINT_DIR, \"final_optimizer.pt\")\n",
        "\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "torch.save(optimizer.state_dict(), final_optimizer_path)\n",
        "\n",
        "print(f\"\\nFinal model saved at: {final_model_path}\")\n",
        "print(f\"Final optimizer saved at: {final_optimizer_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UqgaQQHBCgv",
        "outputId": "1aed24ae-aa78-4143-961c-312d112435a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final model saved at: ./checkpoints/final_model.pt\n",
            "Final optimizer saved at: ./checkpoints/final_optimizer.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Below code is to load your fine-tuned GPT-2 model.\n",
        "# model = GPT2LMHeadModel.from_pretrained(\"gpt-2\")\n",
        "# state_dict = torch.load(final_model_path, map_location=torch.device(\"cpu\"))\n",
        "# model.eval()\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#     model.to(\"cuda\")\n",
        "\n",
        "def generate_text(prompt, temperature=0.3, top_p=0.9, top_k=50):\n",
        "    \"\"\"Generate text continuation given a prompt\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "    output_ids = model.generate(\n",
        "    **inputs,\n",
        "    eos_token_id=tokenizer.eos_token_id, # Generate till transformer sees the EOS token, signalling the end of the sentence\n",
        "    max_length=120,\n",
        "    temperature=0.8, # This controls the randomness of text generated.\n",
        "    top_p=0.9, # Model looks at the smallest set of words whose cumulative probability ≥ 0.9, then samples from that set.\n",
        "    top_k=50, # At each se the model only considers the k most top words\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.2, # To prevent the transformer from generating repetative sentences by imposing a penalty\n",
        "    no_repeat_ngram_size=3, # To prevent transformer from generating repetative trigrams\n",
        "  )\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Both top_p and top_k are to prevent the model from generating nonsense\n",
        "# temperature controls the creativity\n",
        "# repetition_penalty and no_repeat_ngram_size prevents repetitions\n",
        "# If do_sample was false it will always only pick the word with the highest probability. To ensure some variation\n",
        "# we give do_sample = True, and control the variation with top_p and top_k\n",
        "\n",
        "# We need to give a prompt in order to generate a poem\n",
        "prompt = \"The myths\"\n",
        "print(generate_text(prompt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f959iLPZBCuc",
        "outputId": "a70357dd-4719-4087-a8e1-6fb928052f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The myths that were told about my life, and the stories I heard from other people as well. The last time you saw me was in a bar with friends at 6am when we walked down to an abandoned house for lunch; it had been ten years since your birthday—but now one of us has died (I have no memory), so there is some talk among our neighbors on how many more will be left behind if this leaves them alone again after twenty-five or thirty generations: perhaps they are all dead right? And what should happen next would not help either! Maybe even kill each others\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Below code is to load your fine-tuned GPT-2 model.\n",
        "# model = GPT2LMHeadModel.from_pretrained(\"gpt-2\")\n",
        "# state_dict = torch.load(final_model_path, map_location=torch.device(\"cpu\"))\n",
        "# model.eval()\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#     model.to(\"cuda\")\n",
        "\n",
        "def generate_text(prompt, temperature=0.3, top_p=0.9, top_k=50):\n",
        "    \"\"\"Generate text continuation given a prompt\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "    output_ids = model.generate(\n",
        "    **inputs,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    max_length=120,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    top_k=50,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.2,\n",
        "    no_repeat_ngram_size=3,\n",
        "  )\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# We need to give a prompt in order to generate a poem\n",
        "prompt = \"The misty forest\"\n",
        "print(generate_text(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDie3aC1a439",
        "outputId": "2ea69777-9c97-4a57-8fa0-5c408de9c022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The misty forest of the sun-flowered moon, a dark and deep one;—where I am not yet but in my youth. This night is all this restful for me: there are no more hours that lie before them so long as they last? No longer must sleep make us weary by itself or with it be left alone to spend these two nights at ease under some tree above their house where you have stayed many years without any other than your own!\n",
            "\"Oh let him come back from his solitude into our room again!\" said we on each side till he came out laughing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Below code is to load your fine-tuned GPT-2 model.\n",
        "# model = GPT2LMHeadModel.from_pretrained(\"gpt-2\")\n",
        "# state_dict = torch.load(final_model_path, map_location=torch.device(\"cpu\"))\n",
        "# model.eval()\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#     model.to(\"cuda\")\n",
        "\n",
        "def generate_text(prompt, temperature=0.3, top_p=0.9, top_k=50):\n",
        "    \"\"\"Generate text continuation given a prompt\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "    output_ids = model.generate(\n",
        "    **inputs,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    max_length=120,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    top_k=50,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.2,\n",
        "    no_repeat_ngram_size=4,\n",
        "  )\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# We need to give a prompt in order to generate a poem\n",
        "prompt = \"The misty mountains\"\n",
        "print(generate_text(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0065adb1-e9a0-4aac-d0da-0060a8f55945",
        "id": "X7LQ86tFdDTq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The misty mountains, the white of morning-gloom: this is it. The fog that surrounds us—the light so clear in all but our eyes; we must not see what will happen when you go home and have your food or drink for breakfast on Sunday night afternoons? But if I could fly into another world where people are always talking about things like race because they know there's no one else to talk with over here at lunchtime just now a few minutes away from me would be good news! And yet how can anything make my life better than living under such an umbrella\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0bzDERemj8i"
      },
      "source": [
        "Observations:\n",
        "\n",
        "Initially i was feeding title, poem and the tags seperated by [TITLE], [POEM] and [TAGS] special tokens. But the model was struggling to produce any meaningful poems. After only the poems was fed, the performance is much better. It could be due to the fact that the model couldn't correctly learn the meaning of the special tokens. Also initially even longer length poems were being fed, but finally decision was made to cut out very long poems as the gpt-2 model handles shorted sequences better."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}